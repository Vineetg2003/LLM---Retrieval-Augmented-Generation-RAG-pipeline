# 📄 RAG-Powered PDF Question Answering System

## 🧠 Overview

This project implements a **Retrieval-Augmented Generation (RAG)** pipeline that allows users to:

- Upload PDF documents  
- Ask questions based on their content  
- Get accurate answers generated by an LLM  

The system leverages:
- FAISS vector store for fast semantic search  
- Sentence Transformers for embedding  
- A transformer-based LLM (`flan-t5-base`) for answer generation  
- Docker for deployment on local/cloud environments  

---

## ⚙️ Features

✅ PDF Upload with chunking and OCR fallback  
✅ Vector-based retrieval with FAISS  
✅ Context-aware responses using `flan-t5-base`  
✅ FastAPI-based backend  
✅ Dockerized for easy deployment  

---

## 🛠️ Tech Stack

| Layer             | Tool/Library                            |
|------------------|-----------------------------------------|
| Backend API      | FastAPI                                 |
| PDF Processing   | PyPDF2, pdf2image, pytesseract          |
| Embedding        | `all-MiniLM-L6-v2` (Sentence Transformers) |
| Vector Store     | FAISS                                   |
| LLM              | `google/flan-t5-base` (Transformers)    |
| Containerization | Docker + Docker Compose                 |

---

## 📦 Installation

### 🔧 Prerequisites

- Python 3.9+
- Git
- Docker & Docker Compose
- Poppler & Tesseract for PDF OCR

#### 🐧 Ubuntu / Debian:

```bash
sudo apt update
sudo apt install -y poppler-utils tesseract-ocr

# Clone the project
git clone https://github.com/yourusername/rag-pdf-qa.git
cd rag-pdf-qa

# Create and activate a virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Run the FastAPI server
uvicorn app.main:app --reload

# Build and run
docker-compose up --build

.
├── app/
│   ├── routes.py            # API routes
│   ├── chunker.py           # PDF text chunking + OCR
│   ├── retriever.py         # Vector search via FAISS
│   ├── llm.py               # Answer generation with flan-t5
│   ├── metadata_store.py    # Metadata storage
│   └── main.py              # FastAPI app entrypoint
├── faiss_index.bin          # FAISS index (saved)
├── chunks.json              # Saved text chunks
├── Dockerfile
├── docker-compose.yml
├── requirements.txt
└── README.md


## 📈 Performance Note

My system has only **8GB RAM**, so we used the lightweight `flan-t5-base` model.  
For larger models (like `flan-t5-xl` or `mistral-7b-instruct`), increase your chunk size, token limits, and RAM/GPU requirements.

