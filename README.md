# ğŸ“„ RAG-Powered PDF Question Answering System

## ğŸ§  Overview

This project implements a **Retrieval-Augmented Generation (RAG)** pipeline that allows users to:

- Upload PDF documents  
- Ask questions based on their content  
- Get accurate answers generated by an LLM  

The system leverages:
- FAISS vector store for fast semantic search  
- Sentence Transformers for embedding  
- A transformer-based LLM (`flan-t5-base`) for answer generation  
- Docker for deployment on local/cloud environments  

---

## âš™ï¸ Features

âœ… PDF Upload with chunking and OCR fallback  
âœ… Vector-based retrieval with FAISS  
âœ… Context-aware responses using `flan-t5-base`  
âœ… FastAPI-based backend  
âœ… Dockerized for easy deployment  

---

## ğŸ› ï¸ Tech Stack

| Layer             | Tool/Library                            |
|------------------|-----------------------------------------|
| Backend API      | FastAPI                                 |
| PDF Processing   | PyPDF2, pdf2image, pytesseract          |
| Embedding        | `all-MiniLM-L6-v2` (Sentence Transformers) |
| Vector Store     | FAISS                                   |
| LLM              | `google/flan-t5-base` (Transformers)    |
| Containerization | Docker + Docker Compose                 |

---

## ğŸ“¦ Installation

### ğŸ”§ Prerequisites

- Python 3.9+
- Git
- Docker & Docker Compose
- Poppler & Tesseract for PDF OCR

#### ğŸ§ Ubuntu / Debian:

```bash
sudo apt update
sudo apt install -y poppler-utils tesseract-ocr

# Clone the project
git clone https://github.com/yourusername/rag-pdf-qa.git
cd rag-pdf-qa

# Create and activate a virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Run the FastAPI server
uvicorn app.main:app --reload

# Build and run
docker-compose up --build

.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ routes.py            # API routes
â”‚   â”œâ”€â”€ chunker.py           # PDF text chunking + OCR
â”‚   â”œâ”€â”€ retriever.py         # Vector search via FAISS
â”‚   â”œâ”€â”€ llm.py               # Answer generation with flan-t5
â”‚   â”œâ”€â”€ metadata_store.py    # Metadata storage
â”‚   â””â”€â”€ main.py              # FastAPI app entrypoint
â”œâ”€â”€ faiss_index.bin          # FAISS index (saved)
â”œâ”€â”€ chunks.json              # Saved text chunks
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


## ğŸ“ˆ Performance Note

My system has only **8GB RAM**, so we used the lightweight `flan-t5-base` model.  
For larger models (like `flan-t5-xl` or `mistral-7b-instruct`), increase your chunk size, token limits, and RAM/GPU requirements.

